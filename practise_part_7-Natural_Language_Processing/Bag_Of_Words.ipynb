{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiovascular-focus",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powerful-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-embassy",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "patent-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/Users/jayparekh/Documents/machine_learning_dataScience/Machine Learning A-Z (Codes and Datasets)/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing/Python/Restaurant_Reviews.tsv\", delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-mediterranean",
   "metadata": {},
   "source": [
    "# Cleaning Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in dataset.iloc[ : , 0] : \n",
    "    review = re.sub('[^a-zA-z]', ' ', i)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_words = stopwords.words('english')\n",
    "    all_words = all_words.remove('not')\n",
    "    review = [ps.stem(word) for word in review if word not in set(all_words)]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-snapshot",
   "metadata": {},
   "source": [
    "# Creating Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(1500)\n",
    "independent = cv.fit_transform(corpus).toarray()\n",
    "dependent = dataset.iloc[ : , -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-princess",
   "metadata": {},
   "source": [
    "# Splitting The Dataset Into Test Set And Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "independent_train, independent_test, dependent_train, dependent_test = train_test_split(independent, dependent, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-gentleman",
   "metadata": {},
   "source": [
    "# Making Naive Bayes Classification Model On Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(independent_train, dependent_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-finance",
   "metadata": {},
   "source": [
    "# Predicting Test Set Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_pred = classifier.predict(independent_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-lucas",
   "metadata": {},
   "source": [
    "# Visualising Difference In Test Set Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack((dependent_test, dependent_pred), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-scene",
   "metadata": {},
   "source": [
    "# Making Confusion Matrix And Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(dependent_test, dependent_pred)\n",
    "ac = accuracy_score(dependent_test, dependent_pred)\n",
    "print(cm)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-wright",
   "metadata": {},
   "source": [
    "# Predicting Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = \"i do not like this restaurant any more\"\n",
    "new_review = re.sub(new_review)\n",
    "new_review = new_review.lower()\n",
    "new_review = new_review.split()\n",
    "all_words  = stopwords.words('english')\n",
    "all_words  = all_words.remove('not')\n",
    "new_review = [ps.stem(word) for word in new_review if word not in set(all_words)]\n",
    "new_review = \" \".join(new_review)\n",
    "new_inde   = cv.transform([new_review]).toarray()\n",
    "new_depe   = classifier.predict(new_inde)\n",
    "print(new_depe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
